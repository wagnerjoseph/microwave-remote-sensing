[
  {
    "objectID": "unit_02/06_backscatter-variability_exercise.html",
    "href": "unit_02/06_backscatter-variability_exercise.html",
    "title": "RGB Composite",
    "section": "",
    "text": "The workflow is kind of finished. To create the exercise - code could be removed and some “YOUR CODE HERE” could be added - there could be a discussion/explanation text where students could explain what they see. Therefore they could select their own area and time."
  },
  {
    "objectID": "unit_02/06_backscatter-variability_exercise.html#load-sentinel-1-data",
    "href": "unit_02/06_backscatter-variability_exercise.html#load-sentinel-1-data",
    "title": "RGB Composite",
    "section": "Load Sentinel 1 data",
    "text": "Load Sentinel 1 data\n\n# SAT-1 data is inside the saocom_gamma folder but this could change\ndata_path = Path(\n    '~/shared/datasets/rs/datapool/mrs/saocom_gamma/S1gammaNeusiedler'\n    ).expanduser()\n\ntif_files = glob(f\"{data_path}/*.tif\")\n\n\ndef _preprocess(x):\n    scale_factor = pd.to_numeric(\n        rasterio.open(x.encoding[\"source\"]).tags().get('scale_factor')\n    )\n    x = x / scale_factor\n\n    time_value = pd.to_datetime(\n        rasterio.open(x.encoding[\"source\"]).tags().get('time_begin')\n    )\n    x = x.assign_coords(time=time_value).expand_dims(\"time\")\n\n    return x.rename(\n        {\"band_data\": \"sig0\"}\n    ).squeeze(\"band\").drop_vars(\"band\")\n\n\nsig0_ds_large = xr.open_mfdataset(\n    '~/shared/datasets/rs/datapool/mrs/saocom_gamma/S1gammaNeusiedler/*.tif',\n    engine=\"rasterio\",\n    combine='nested',\n    concat_dim=\"time\",\n    preprocess=_preprocess\n)\n\nsig0_ds_large = sig0_ds_large[list(sig0_ds_large.data_vars.keys())[0]]\nsig0_ds_large\n\nLets select a smaller region. Try to use a different area than in the Notebook 06.\n\nbounding_box = {\n\n    # YOUR CODE HERE ----------------------------------------------------------\n    'x_min': 5_265_000.0,\n    'x_max': 5_295_000.0,\n    'y_min': 1_550_000.0,\n    'y_max': 1_570_000.0\n    # -------------------------------------------------------------------------\n    \n}\n\n\nimg = sig0_ds_large.isel(time=0) .coarsen(x=10, y=10, boundary='pad').median()\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nimg.plot(ax=ax, robust=True, cmap=\"viridis\")\n\nrect = mpatches.Rectangle(\n    (bounding_box['x_min'], bounding_box['y_min']),\n    bounding_box['x_max'] - bounding_box['x_min'],\n    bounding_box['y_max'] - bounding_box['y_min'],\n    linewidth=2,\n    edgecolor='red',\n    facecolor='none'\n)\nax.add_patch(rect)\nax.set_aspect('equal')\n\nplt.show()\n\n\nsig0_ds = sig0_ds_large.sel(\n    x=slice(bounding_box['x_min'], bounding_box['x_max']),\n    y=slice(bounding_box['y_max'], bounding_box['y_min'])\n)\nsig0_ds\n\n\nSelect the dates of interest\nNow we want to select three differnt timezones which have a large distance to each other. Here it is also interesting to look at the time of the year. Especially plants can change over the course of the year. We select three different dates for our analysis.\n\nApproach 1:\nHere we take the first the last and one image from the middle.\n\nds = sig0_ds.isel(time=[0, len(sig0_ds.time) // 2, -1])\nds\n\n\n\nApproach 2:\nHere we use nearest neighbour with .sel to get the dates form specific time of the year.\n\nfirst = pd.Timestamp(\"2023-8-1\")\nsecond = pd.Timestamp(\"2023-9-15\")\nthird = pd.Timestamp(\"2023-10-28\")\n\nds = sig0_ds.sel(time=[first, second, third], method='nearest')\n\n\n\n\nMap the backscatter to RGB values\nWe have the data in dB.\nWe want to have the data as values between 0 and 1 so we can match it to the colors.\nWe assume that the same values have the same intensity for red, green and blue\nHow can we achive it:\n\nConvert dB to linear scale.\nGet min and max backscatter of all values of the linear backscatter values. Here the quartiles need to be used to make it robust.\nMap it to the interval [0,1] by calculating by normalised = (value-min)/(max-min) and map it to the coresponding color.\n\n\ndef lin2db(val):\n    return 10 * np.log10(val)\n\n\ndef db2lin(val):\n    return 10 ** (val / 10)\n\n\ndef normalize(val, min, max):\n    return (val - min) / (max - min)\n\n\n\nAttempt to use min and max for Normalization:\n\nlinear_ds = db2lin(ds).compute()\n\nmin_value = linear_ds.min().item()\nmax_value = linear_ds.max().item()\n\nnormalized_ds = normalize(linear_ds, min_value, max_value)\nnormalized_ds.isel(time=0).plot.imshow()\n\nA few of the really high backscattering values are a problem in the normalisation and make the image nearly completely black.\n\n\nRobust Normalization using the 2nd and 98th percentile\nWe want to mapp the values that are under 0 and over 1 after the normalisation to 0 and 1.\n\nmin_robust = linear_ds.quantile(0.02).item()\nmax_robust = linear_ds.quantile(0.98).item()\n\nnormalized_ds = normalize(linear_ds, min_value, max_robust).clip(min=0, max=1)\nnormalized_ds.isel(time=0).plot.imshow()\n\n\n\nPlotting\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nnormalized_ds.isel(time=0).plot(ax=axes[0], cmap='Reds')\nnormalized_ds.isel(time=1).plot(ax=axes[1], cmap='Greens')\nnormalized_ds.isel(time=2).plot(ax=axes[2], cmap='Blues')\n\naxes[0].set_aspect('equal')\naxes[1].set_aspect('equal')\naxes[2].set_aspect('equal')\n\n\nplt.tight_layout()\nplt.show()\n\n\nrgb_image = xr.concat([\n    normalized_ds.isel(time=0),\n    normalized_ds.isel(time=1),\n    normalized_ds.isel(time=2)\n    ],\n    dim='color'\n)\n\nrgb_image = rgb_image.rename(\n    {'color': 'RGB'}\n    ).assign_coords(RGB=['red', 'green', 'blue'])\n\nfig, ax = plt.subplots(figsize=(7, 7))\nrgb_image.plot.imshow()\n\nax.set_aspect('equal')\nplt.tight_layout()\n\nplt.show()\n\n\n\nExplain the plot above."
  },
  {
    "objectID": "unit_02/06_backscatter-variability.html",
    "href": "unit_02/06_backscatter-variability.html",
    "title": "Dielectric Properties of Natural Media",
    "section": "",
    "text": "In this notebook we want to look at how different surfaces like water, forest grasland and cities give us different backscatter.\nWe are going to load backscatering data from Sentinel 1 and we will use the Corine Landcoverdata to get a classification of different surfaces.\n# Imports\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport rasterio\nimport json\nimport os\nfrom glob import glob\nfrom pprint import pprint\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nfrom pyproj import Transformer\nimport holoviews as hv\nfrom holoviews.streams import RangeXY"
  },
  {
    "objectID": "unit_02/06_backscatter-variability.html#load-sentinel-1-data",
    "href": "unit_02/06_backscatter-variability.html#load-sentinel-1-data",
    "title": "Dielectric Properties of Natural Media",
    "section": "Load Sentinel 1 data",
    "text": "Load Sentinel 1 data\nLoad Metadata to get some information on the data. Using ‘.tags()’ we can extract a lot of useful information like the scale_factor.\nAs we can see frome here the data was captured 8 images between the 5th August 2023 and the 28th October 2023. The image contains Gamma VV values.\nNow we can load the data and and apply preprocessing. From the Metaddata we could extract a scalingfactor of 100 which we now need to apply.\nWe get a xarray.DataSet and convert it to an xarray.DataArray because we only have one variable, the VV backscatter.\nWe have 8 timestemps. For performance reasons we will fous on only on one time becasue the data does not change to much over time.\nWe downsample already in the preprocessing because we do not need all the resolution because we are working with the corine data map and the resolution there is 100 by 100 meters. When we downsample x and y coordinates by 10 we get pixles of size 100 by 100meters which fits perfectly to the\n(insert image of downscaled, normal and corine landcover)\n\n# SAT-1 data is inside the saocom_gamma folder but this could change\ndata_path = Path(\n    '~/shared/datasets/rs/datapool/mrs/saocom_gamma/S1gammaNeusiedler'\n    ).expanduser()\n\ntif_files = glob(f\"{data_path}/*.tif\")\n\n\ndef _preprocess(x):\n    file = x.encoding['source']\n\n    with rasterio.open(file) as src:\n\n        print(f\"{os.path.basename(file)}: \")\n        print(json.dumps(src.tags(), indent=4))\n        meta = src.meta.copy()\n        meta['crs'] = str(meta['crs'])\n        print(json.dumps(meta, indent=4))\n\n        scale_factor = pd.to_numeric(src.tags().get('scale_factor'))\n        x = x / scale_factor\n\n        time_value = pd.to_datetime(src.tags().get('time_begin'))\n        x = x.assign_coords(time=time_value).expand_dims(\"time\")\n\n    return x.rename(\n        {\"band_data\": \"sig0\"}\n    ).squeeze(\"band\").drop_vars(\"band\")\n\n\nsig0_ds_large = xr.open_mfdataset(\n    '~/shared/datasets/rs/datapool/mrs/saocom_gamma/S1gammaNeusiedler/*.tif',\n    engine=\"rasterio\",\n    combine='nested',\n    concat_dim=\"time\",\n    preprocess=_preprocess\n)\n\nsig0_ds_large = sig0_ds_large[list(sig0_ds_large.data_vars.keys())[0]]\nsig0_ds_large\n\nWe print the metadata to get some feeling of which data we are actually using. The data we are using\n\nbounding_box = {\n    'x_min': 5_280_000.0,\n    'x_max': 5_295_000.0,\n    'y_min': 1_560_000.0,\n    'y_max': 1_570_000.0\n}\n\nWe corase 10 by 10 pixles to one large one to make the plotting faster. Here we use the median for the coarsing because the mean would be mathematicaly not correct as we learned in Notebook 02.\n\nimg = sig0_ds_large.isel(time=0) .coarsen(x=10, y=10, boundary='pad').median()\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nimg.plot(ax=ax, robust=True, cmap=\"viridis\")\n\nrect = mpatches.Rectangle(\n    (bounding_box['x_min'], bounding_box['y_min']),\n    bounding_box['x_max'] - bounding_box['x_min'],\n    bounding_box['y_max'] - bounding_box['y_min'],\n    linewidth=2,\n    edgecolor='red',\n    facecolor='none'\n)\nax.add_patch(rect)\nax.set_aspect('equal')\n\nplt.show()\n\n\nsig0_ds = sig0_ds_large.sel(\n    x=slice(bounding_box['x_min'], bounding_box['x_max']),\n    y=slice(bounding_box['y_max'], bounding_box['y_min'])\n)\nsig0_ds\n\nTo get an idea of the region we are looking at we want to display it. Using ‘rubust=True’ will use the 2nd and 98th percentiles of the data to compute the color limits.\n\nsig0_ds.isel(time=0).plot(robust=True).axes.set_aspect('equal')"
  },
  {
    "objectID": "unit_02/06_backscatter-variability.html#load-corine-landcover-data",
    "href": "unit_02/06_backscatter-variability.html#load-corine-landcover-data",
    "title": "Dielectric Properties of Natural Media",
    "section": "Load Corine Landcover data",
    "text": "Load Corine Landcover data\nWe load the Corine Land Cover data which classifies the landcover of europa in 44 different areas.\nWe want to work with the Equi7 grid because it preserves geometric accuracy and minimises data oversampling over surfaces. So we need to convert the Corine Land Cover map to this coordinate system.\n\nLoading data\n\ncorine_path = Path(\n    '~/shared/datasets/rs/datapool/mrs/Corine_Land_Cover_2018.tif'\n    ).expanduser()\n\npprint(\n    f\"Coordinate Reference System: {rasterio.open(corine_path).meta['crs']}\"\n    )\n\ncorine_ds_large = (\n    xr.open_dataset(corine_path, engine=\"rasterio\")\n    .rename({\"band_data\": \"land_cover\"})[\"land_cover\"]\n    .squeeze()\n)\n\ncorine_ds_large\n\n\n\nSelect area of interest\n\ntransformer = Transformer.from_crs(\"EPSG:27704\", \"EPSG:3035\", always_xy=True)\n\nbbox_27704 = sig0_ds_large.rio.transform_bounds(sig0_ds_large.rio.crs)\nbbox_3035 = transformer.transform_bounds(*bbox_27704)\n\nprint(bbox_27704)\nprint(bbox_3035)\n\nx_min, y_min, x_max, y_max = bbox_3035\ncorine_ds = corine_ds_large.sel(x=slice(x_min, x_max), y=slice(y_max, y_min))\n\n\n\nAlignment of the datasets (raster + pixles)\n\ndef print_details(ds, ds_name):\n    print(f\"{ds_name}: \")\n    print(f\"CRS: {ds.rio.crs}\")\n    print(f\"Bounds: {ds.rio.bounds()}\")\n    print(f\"Pixel size: {ds.rio.resolution()}\\n\")\n\n\nprint_details(corine_ds, \"corine_ds\")\nprint_details(sig0_ds, \"sig0_ds\")\n\nReprojecting does a whole lot for us\n\ncorine_ds = corine_ds.rio.reproject_match(sig0_ds)\n\nprint_details(corine_ds, \"corine_ds\")\nprint_details(sig0_ds, \"sig0_ds\")\n\n\n\nColormapping and Encoding\nThe color mapping is stored in a json file under assets. The encoding can be found here: https://collections.sentinel-hub.com/corine-land-cover/readme.html\n\nwith open('../assets/06_color_mapping.json', 'r') as f:\n    color_mapping_data = json.load(f)\n\ncolor_mapping = {\n    item['value']: item for item in color_mapping_data['land_cover']\n    }\n\ncolors = [info['color'] for info in color_mapping.values()]\ncategories = [info['value'] for info in color_mapping.values()]\n\ncmap = ListedColormap(colors)\nnorm = BoundaryNorm(categories + [max(categories) + 1], len(categories))\n\n\nmax_length = 50\nindentation = ' ' * 7\n\npresent_landcover_codes = np.unique(\n    corine_ds.values[~np.isnan(corine_ds.values)].astype(int)\n    )\n\nhandles = [\n    mpatches.Patch(\n        color=info['color'],\n        label=(\n            f'{info[\"value\"]} - ' +\n            (info[\"label\"] if len(info[\"label\"]) &lt;= max_length\n             else f'{info[\"label\"][:max_length]}-\\n{indentation}{info[\"label\"][max_length:]}')\n        )\n    )\n    for info in color_mapping.values()\n    if info['value'] in present_landcover_codes\n]\n\ncorine_ds.plot(\n    figsize=(10, 10),\n    cmap=cmap,\n    norm=norm,\n    add_colorbar=False).axes.set_aspect('equal')\n\nplt.legend(\n    handles=handles,\n    bbox_to_anchor=(1.05, 1),\n    loc='upper left',\n    borderaxespad=0.,\n    fontsize=7)\nplt.title(\"Corine Land Cover (EPSG:27704)\")\nplt.xlabel(\"x coordinates [meters]\")\nplt.ylabel(\"y coordinates [meters]\")\nplt.show()\n\nNow we can combine the datasets to one xarray dataset.\n\nfs = xr.merge([sig0_ds, corine_ds]) .drop_vars(\"band\") .compute()\nfs\n\n\n\nCreating a Histogram for a specific landcover backscatter\n\nwaterbodies_mask = (fs.land_cover == 41)  # 41 = encoded value for water bodies\nwaterbodies_mask.plot().axes.set_aspect('equal')\n\n\nwaterbodies_sig0 = fs.sig0.isel(time=0).where(waterbodies_mask)\nwaterbodies_sig0.plot(robust=True).axes.set_aspect('equal')\n\n\nwaterbodies_sig0.plot.hist(bins=50, edgecolor='black');\n\n\n\nDashboard backscatter of different landcover over time\n\nhv.extension('bokeh')\n\n# ---------------\n\nrobust_min = fs.sig0.quantile(0.02).item()\nrobust_max = fs.sig0.quantile(0.98).item()\n\nbin_edges = [\n    i + j * 0.5\n    for i in range(int(robust_min) - 2, int(robust_max) + 2)\n    for j in range(2)\n]\n\nland_cover = {\"\\xa0\\xa0\\xa0\\xa0 Complete Land Cover\": 1}\nland_cover.update({\n    f\"{int(value): 02} {color_mapping[value]['label']}\": int(value)\n    for value in present_landcover_codes\n})\ntime = fs.sig0['time'].values\n\nrangexy = RangeXY()\n\n# ---------------\n\n\ndef load_image(time, land_cover, x_range, y_range):\n\n    if land_cover == \"\\xa0\\xa0\\xa0\\xa0 Complete Land Cover\":\n        sig0_selected_ds = fs.sig0.sel(time=time)\n\n    else:\n        land_cover_value = int(land_cover.split()[0])\n        mask_ds = (fs.land_cover == land_cover_value)\n        sig0_selected_ds = fs.sig0.sel(time=time).where(mask_ds)\n\n    hv_ds = hv.Dataset(sig0_selected_ds)\n    img = hv_ds.to(hv.Image, [\"x\", \"y\"])\n\n    if x_range and y_range:\n        img = img.select(x=x_range, y=y_range)\n    return hv.Image(img).opts(\n        cmap=\"viridis\", colorbar=True, frame_width=368,\n        tools=[\"hover\"], clim=(robust_min, robust_max), aspect=\"equal\")\n\n\ndmap = hv.DynamicMap(\n    load_image,\n    kdims=[\"Time\", \"Landcover\"],\n    streams=[rangexy]\n).redim.values(\n    Time=time,\n    Landcover=land_cover\n).hist(\n    normed=True,\n    bins=bin_edges\n)\n\ndmap = dmap.opts(framewise=True, show_title=True)\ndmap"
  },
  {
    "objectID": "unit_01/02_unit-conversions.html",
    "href": "unit_01/02_unit-conversions.html",
    "title": "Unit Conversion",
    "section": "",
    "text": "In this notebook we are gonna have a look at the conversion of units. The Sentinel-1 data that is mostly used in this exercise is SAR data, which is measured in decibels (dB). In order to make meaningful calculations with the data, we need to convert the data to linear scale. Lets start with importing some libraries.\n# Imports\nimport numpy as np\nimport pystac_client\nimport odc.stac\nimport matplotlib.pyplot as plt\nfrom odc.geo.geobox import GeoBox\nfrom dask.diagnostics import ProgressBar\nfrom rasterio.crs import CRS\nimport seaborn as sns"
  },
  {
    "objectID": "unit_01/02_unit-conversions.html#backscattering-coefficients",
    "href": "unit_01/02_unit-conversions.html#backscattering-coefficients",
    "title": "Unit Conversion",
    "section": "Backscattering Coefficients",
    "text": "Backscattering Coefficients\nLets have a quick look at the different backscattering coefficients that are used in SAR data processing: - radar brightness or beta nought \\(\\beta^0\\): Is the original backscatter value related to the measurement geometry (range and azimuth). Thus, it is a quantity being independent from terrain variations (and hence also incidence angles). - sigma nought \\(\\sigma^0\\): Is the backscatter value related to the illuminated ground-area. It takes the influence of terrain variations causing a variation in illumination into account, by scaling \\(\\beta^0\\) with \\(sin(\\theta_i)\\), where \\(\\theta_i\\) is the local incidence angle. - gamma nought \\(\\gamma^0\\): Is the backscatter value related to the area perpendicular to the looking direction. It can be computed by normalising/scaling \\(\\beta^0\\) with \\(1/tan(\\theta_i)\\)"
  },
  {
    "objectID": "unit_01/02_unit-conversions.html#scale",
    "href": "unit_01/02_unit-conversions.html#scale",
    "title": "Unit Conversion",
    "section": "Scale",
    "text": "Scale\nThe decibel (dB) is a logarithmic unit used to express the ratio of two values of a physical quantity, often power or intensity. In the case of SAR data, the backscatter coefficient is often expressed in dB. The reason for that will be explained in the following example."
  },
  {
    "objectID": "unit_01/02_unit-conversions.html#mathematical-operation",
    "href": "unit_01/02_unit-conversions.html#mathematical-operation",
    "title": "Unit Conversion",
    "section": "Mathematical operation",
    "text": "Mathematical operation\nIn order to convert the data from dB to linear scale, we need to convert the data using the following formula. Let \\(x\\) be the original value (dB) and \\(y\\) the converted value (\\(m^2m^{-2}\\)). The conversion of units can be expressed as: \\[\nD =  10  \\cdot \\log_{10} (I) = 10 \\cdot \\log_{10} (e) \\cdot \\ln (I)\\longrightarrow [dB]\n\\] Similarly, the conversion back to the original unit can be expressed as: \\[\nI = e^{\\frac{D}{10\\cdot \\log_{10}(e)}} = 10^{\\frac{D}{10}} \\longrightarrow [m^2m^{-2}]  \n\\] You can find these formulas in the script for Microwave Remote Sensing on page 136 (equation 6.40).\nNow lets implement the conversion in Python.\n\n# Conversion functions\ndef lin2db(val:float|int) -&gt; float:\n    '''\n    Converts value from linear to dB units.\n\n    Parameters\n    ----------\n    x : number\n        Value in linear units.\n\n    Returns\n    -------\n    float\n        Value in dB.\n    '''\n    return 10 * np.log10(val)\n\ndef db2lin(val:float|int) -&gt; float:\n    '''\n    Converts value from dB to linear units.\n\n    Parameters\n    ----------\n    x : number\n        Value in dB.\n\n    Returns\n    -------\n    float\n        Value in linear units.\n    '''\n    return 10 ** (val / 10)\n\nThe reason why we convert the data to linear scale is that when values in dB are added or subtracted, the actual values are multiplied or divided. This is not the case when the values are in linear scale. Lets have a look at an example, where we add two values in once without the conversion to linear scale and once with the conversion to linear scale.\n\n# Logarithmic addition\n# Values in linear and decibel units\nval1_db, val2_db = 10, 12\n\n# Logarithmic addition\nsum_db = val1_db + val2_db\nprint(f'Logarithmic Addition:')\nprint(f'Logarithmic values: \\t{val1_db:&lt;5}, {val2_db:&lt;5} [dB]')\nprint(f'Logarithmic sum: \\t{val1_db} + {val2_db} = {sum_db:&lt;5}')\n\n# Linear addition\nval1_lin, val2_lin = db2lin(val1_db), db2lin(val2_db)\nsum_lin = val1_lin + val2_lin\nprint(f'\\nLinear Addition:')\nprint(f'Linear values: \\t\\t{val1_lin:&lt;5}, {val2_lin:&lt;5.2f} [lin] (converted from dB)')\nprint(f'Linear sum: \\t\\t{val1_lin} + {val2_lin:.2f} = {sum_lin:.2f} [lin]')\nprint(f'\\t\\t\\t= {lin2db(sum_lin):.2f} [dB]')\n\nprint(f'\\nValues compared in dB: \\t{sum_db} [dB] != {lin2db(sum_lin):.2f} [dB] (difference = {sum_db - lin2db(sum_lin):.2f} [dB])')\nprint(f'Values compared in lin: {db2lin(sum_db):.2f} [lin] != {sum_lin:.2f} [lin] (factor = {db2lin(sum_db) / sum_lin:.2f})')\n\nAs you can see the values in dB and in linear scale differ quite a bit. In the example above the values differ by a factor of around 6 when looked at in linear scale. This is why it is important to convert the data to linear scale before doing any calculations. Lets load some sample data and see what the difference is between the two scales and why we use dB in the first place."
  },
  {
    "objectID": "unit_01/02_unit-conversions.html#example-usage",
    "href": "unit_01/02_unit-conversions.html#example-usage",
    "title": "Unit Conversion",
    "section": "Example Usage",
    "text": "Example Usage\n\nLoad some sample data\nIn order to demonstrate why this conversion is important, we will have a look at some SAR data.\n\n# Search for some data\n\nepsg = CRS.from_epsg(4326) # WGS 84 (only integer would also work)\ndx = 0.0002 # degrees\n\n# Set Spatial extent\nlatmin, latmax = 48, 48.5\nlonmin, lonmax = 16, 17\nbounds = (lonmin, latmin, lonmax, latmax)\nminx, miny, maxx, maxy = bounds\ngeom = {\n    'type': 'Polygon',\n    'coordinates': [[\n       [minx, miny],\n       [minx, maxy],\n       [maxx, maxy],\n       [maxx, miny],\n       [minx, miny]\n    ]]\n}\n\n# Set Temporal extent\ntime_range = \"2022-08-01/2022-08-05\" # closed range\n\n# Search for Sentinel-2 data\nitems = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\").search(\n    # intersects=geom,\n    bbox=bounds,\n    collections=[\"SENTINEL1_SIG0_20M\"],\n    datetime=time_range,\n    limit=100,\n).item_collection()\n\nprint(len(items), 'scenes found')\n\n\n# Load the data into a Dask array\n\n# define a geobox for my region\ngeobox = GeoBox.from_bbox(bounds, crs=epsg, resolution=dx)\n\n# lazily combine items\nsig0_dc = odc.stac.stac_load(\n    items,\n    #bbox=bounds,\n    bands=[\"VV\", \"VH\"],\n    chunks={'time': 5, 'x': 600, 'y': 600},\n    epsg=epsg,\n    geobox=geobox,\n    resampling=\"bilinear\",\n)\n\n\n# Define the nodata value and scale factor\nnodata = -9999  # Sentinel-1 nodata value as defined by EODC\nscale = 0.1     # Sentinel-1 scale factor as defined by EODC\n\n# Preprocess the data\ntemp_mean = (sig0_dc.where(sig0_dc != nodata)*scale).VV.median(dim='time')\n\n\naoi = temp_mean.sel(latitude=slice(48.3, 48.2), longitude=slice(16.4, 16.6))\ndB_data = aoi.values.flatten()\nlin_data = db2lin(dB_data)\n\nfig, ax = plt.subplots(2,2, figsize=(10,8))\n#upper left\naoi.plot.imshow(robust=True, ax=ax[0,0])\nax[0,0].set_title('Sentinel-1 backscatter $\\sigma_0$ [$dB$]')\n\n#upper right\nsns.violinplot(data=dB_data, ax=ax[0,1])\nax[0,1].set_ylabel('$\\sigma_0$ [$dB$]')\nax[0,1].set_title('Sentinel-1  $\\sigma_0$ [$dB$] distribution')\n\n#lower left\ndb2lin(aoi).plot.imshow(robust=True, ax=ax[1,0])\nax[1,0].set_title('Sentinel-1 backscatter $\\sigma_0$ [$m^2 \\cdot m^{-2}$]')\n\n\n#lower right\nsns.violinplot(data=lin_data, ax=ax[1,1])\nax[1,1].set_ylabel('$\\sigma_0$ [$m^2 \\cdot m^{-2}$]')\nax[1,1].set_title('Sentinel-1 $\\sigma_0$ [$m^2 \\cdot m^{-2}$] distribution')\nplt.tight_layout()\n\nIn the plot above you can see the difference between the two scales. The values in dB are more evenly distributed and are therefore easier to plot. The values in linear scale are more spread out and are therefore harder to interpret. This is why we use the dB scale for plotting and the linear scale for calculations.\nNow that we have some data, we will try to calculate the average \\(\\sigma_0\\) value across the scene. We will do this by converting the data to linear scale, calculating the average and converting it back to dB.\n\n# Lets take a data array with db values\ndb_array = temp_mean.compute()\n\n# Compute the linear values\nlin_array = db2lin(db_array)\n\n\n# Compute the average backscatter value in linear units across the whole scene\nlin_mean = lin_array.mean()\nprint(f\"Average backscatter value in linear units: {lin_mean.values:.3f}\")\ndb_from_lin_mean = lin2db(lin_mean)\nprint(f\"That value in dB: {db_from_lin_mean.values:.3f}\\n\")\n\n# Compute the average backscatter value in dB across the whole scene\ndb_mean = db_array.mean()\nprint(f\"Average backscatter value in dB: {db_mean.values:.3f}\")\n\nAs you can see in the example the mean values across the scene are different in dB and linear scale. This is why it is important to convert the data to linear scale before doing any calculations. It is usually not meaningful to perform calculations on data in dB. Instead, it is better to convert the data to linear scale, perform the calculations and convert it back to dB."
  },
  {
    "objectID": "unit_01/01_discover-and-read.html",
    "href": "unit_01/01_discover-and-read.html",
    "title": "Discover Sentinel 1 data using the EODC STAC catalog",
    "section": "",
    "text": "This notebook demonstrates how to access radar data in a STAC Catalogue using the pystac library. In this examples we use here Sentinel-1 data on the EODC stac catalog. In the further process we will learn how to query a STAC catalog, select specific items and display their metadata.\nimport pystac_client\nimport folium\nfrom odc import stac as odc_stac\neodc_catalog = pystac_client.Client.open(\n    \"https://stac.eodc.eu/api/v1\"\n    )\n\neodc_catalog\nEach catalog, composed by different providers, has many collections. To get all collections of a catalog, we can print all of them and their ids, which are used to fetch them from the catalog.\ncollections = eodc_catalog.get_collections()\n\n# length of string of collection.id, for pretty print\nmax_length = max(len(collection.id) for collection in collections)\n\nfor collection in eodc_catalog.get_collections():\n    print(f\"{collection.id.ljust(max_length)} : {collection.title}\")\nTo get a specific collection from the catalog, we can use the client.get_collection() method and provide the collection name. We can then display its description, id, temporal and spatial extend, licence, etc.\ncolllection_id = 'SENTINEL1_SIG0_20M'\n\ncollection = eodc_catalog.get_collection(colllection_id)\ncollection\nEach collection has multiple items. An item is one spatio-temporal instance the collection, for instance a satellite image. If items are needed for a specific timeframe or for a specific region of interest, we can define this as a query.\ntime_range = \"2022-10-01/2022-10-07\"  # a closed range\n# time_range = \"2022-01\"  # whole month, same can be done for a year and a day\n# time_range = \"2022-01-01/..\"  # up to the current date, an open range\n# time_range = \"2022-01-01T05:34:46\"  # a specific time instance\nA spatial region of interest can be defined in different ways. One option is to define a simple bounding box:\nTo make the code cleaner, the following bounding box could be used, but generally any area of the world can be selected as polygon (does not have to be a rectangle) with geojson.io and interescts.\nlatmin, latmax = 46.3, 49.3  # South to North\nlonmin, lonmax = 13.8, 17.8  # West to East\n\nbounding_box = [lonmin, latmin, lonmax, latmax]\nIf the region of interest is not rectengular, we can also define a polygon:\n# GEOJSON can be created on geojson.io\n\n# This specific area of interest is a rectangle, but since it is\n# a closed polygon it seems like it has five nodes\n\narea_of_interest = {\n    \"coordinates\": [\n          [\n            [\n              17.710928010825853,\n              49.257630084442496\n            ],\n            [\n              13.881798300915221,\n              49.257630084442496\n            ],\n            [\n              13.881798300915221,\n              46.34747715326259\n            ],\n            [\n              17.710928010825853,\n              46.34747715326259\n            ],\n            [\n              17.710928010825853,\n              49.257630084442496\n            ]\n          ]\n        ],\n        \"type\": \"Polygon\"\n        }\nUsing our previously loaded STAC catalog, we can now search for items fullfilling our query. In this example we are using the bounding box. If we want to use an area of interest specified in the geojson format - one hast to use the intersects parameter as documented in the comment below.\nsearch = eodc_catalog.search(\n    collections=colllection_id,  # can also be a list of several collections\n    bbox=bounding_box,  # search by bounding box\n    # intersects=area_of_interest,  # GeoJSON search\n    datetime=time_range,\n    # max_items = 1  # number of max items to load\n    )\n\n# If we comment everything besides colllection_id, we will load whole\n# collection for available region and time_range\n\nitems_eodc = search.item_collection()\nprint(f\"On EODC we found {len(items_eodc)} items for the given search query\")\nNow, we can fetch a single item, in this case a Sentinel-1 image, from the query results. A good practice is to always check what metadata the data provider has stored on the item level.\nitem = items_eodc[0]\nitem\nLets display only VV polarisation of the item and some information about the data.\nitem.assets['VV'].extra_fields.get('raster:bands')[0]\nIn the EODC STAC catalogue an item can conveniently be displayed using its thumbnail.\nitem.assets['thumbnail'].href\nHere is a way to quickly check how the data found by a search query looks on a map.\nmap = folium.Map(location=[(latmin+latmax)/2, (lonmin+lonmax)/2],\n                 zoom_start=7,\n                 zoom_control=False,\n                 scrollWheelZoom=False,\n                 dragging=False)\n\nfolium.GeoJson(area_of_interest, name=\"Area of Interest\").add_to(map)\n\nfor item in items_eodc:\n    # url leading to display of an item, can also be used as hyperlink\n    image_url = item.assets['thumbnail'].href\n    bounds = item.bbox\n    folium.raster_layers.ImageOverlay(\n        image=image_url,\n        bounds=[[bounds[1], bounds[0]],\n                [bounds[3], bounds[2]]],  # bounds of item\n                ).add_to(map)\n\nfolium.LayerControl().add_to(map)\n\nmap"
  },
  {
    "objectID": "unit_01/01_discover-and-read.html#data-reading",
    "href": "unit_01/01_discover-and-read.html#data-reading",
    "title": "Discover Sentinel 1 data using the EODC STAC catalog",
    "section": "Data Reading",
    "text": "Data Reading\nSTAC can also be a useful tool for the discovery of data, however it only loads metadata. This saves memory, but if one would like to do further analysis the data has to be loaded into memory or downloaded on disk.\nThe library we use here for loading the data is called odc-stac.\n\nbands = (\"VV\", \"VH\")\ncrs = \"EPSG:4326\"  # Coordinate Reference System -\n# World Geodetic System 1984 (WGS84) in this case\nres = 0.00018  # 20 meter in degree\n\nsig0_ds = odc_stac.load(items_eodc,\n                        bands=bands,\n                        crs=crs,\n                        chunks={'time': 1,\n                                'latitude': 1000,\n                                'longitude': 1000},\n                        resolution=res,\n                        bbox=bounding_box\n                        )\n\nLets display for example the VV band of the dataset.\n\nsig0_ds.VV\n\nDisplayed data is “lazily” loaded, which means that structure of DataArray is constructed, but data is not loaded yet. It is loaded only at instance when it is needed, for example plotting, computing and so on. Xarray is convenient for multidimensional labeled arrays, like temperature, humidity, pressure, different bands of satellite imagery and so on. The link to the xarray documentation.\nNevertheless, it is better to choose smaller area, since the array is too big.\n\nlatmin_smaller, latmax_smaller = 48, 48.4\nlonmin_smaller, lonmax_smaller = 16, 16.5\n\nsmaller_bounding_box = [[latmin_smaller, lonmin_smaller],\n                        [latmax_smaller, lonmax_smaller]]\n\nmap = folium.Map(location=[(latmin_smaller+latmax_smaller)/2,\n                           (lonmin_smaller+lonmax_smaller)/2],\n                           zoom_start=8,\n                 zoom_control=False,\n                 scrollWheelZoom=False,\n                 dragging=False)\n\nfolium.GeoJson(area_of_interest, name=\"Area of Interest\").add_to(map)\n\nfolium.Rectangle(\n    bounds=smaller_bounding_box,\n    color=\"red\",\n    ).add_to(map)\n\nfor item in items_eodc:\n    image_url = item.assets['thumbnail'].href\n    bounds = item.bbox\n    folium.raster_layers.ImageOverlay(\n        image=image_url,\n        bounds=[[bounds[1], bounds[0]], [bounds[3], bounds[2]]],\n        ).add_to(map)\n\nfolium.LayerControl().add_to(map)\n\nmap\n\n\nsig0_ds = odc_stac.load(items_eodc,\n                        bands=bands,\n                        crs=crs,\n                        chunks={'time': 1,\n                                'latitude': 1000,\n                                'longitude': 1000},\n                        resolution=res,\n                        bbox=[lonmin_smaller, latmin_smaller,\n                              lonmax_smaller, latmax_smaller],\n                        # groupby='time'\n                        )\n\nsig0_ds.VV\n\nNow dataset is considerably smaller. Lets plot first time instance.\n\nsig0_ds.VV.isel(time=0).plot()\n\nDue to the way the data is acquired and stored, some items include “no data” areas. In our case, no data has the value -9999, but this can very from data provider to data provider. This information can usually be found in the metadata. Furthermore, to save the memory, data is often stored as integer (e.g. 25) and not at float (e.g. 2.5) format. For this reason, the backscatter values are often multiplied by a scale factor, in this case the factor 10.\nAs Sentinel-1 satellites overpasses Austria every few days and does acquisition of backscatter for few minutes, only some part of dataset will have physical data. So it is better to take daily averages (average scene that Sentinel observed for few minutes of overpass) and to see how it looks over whole week (that was previously defined in STAC search).\nNote: cell belows will take some time to run.\n\n# raster:bands is STAC raster extension\nscale = item.assets['VV'].extra_fields.get('raster:bands')[0]['scale']\nnodata = item.assets['VV'].extra_fields.get('raster:bands')[0]['nodata']\n\nsig0_ds = sig0_ds.where(sig0_ds!=nodata) / scale\n\n# We should remove unnecessary dates when there was no dataa\n# (no satellite overpass)\nsig0_ds = sig0_ds.dropna(dim='time')\n\n\nsig0_ds.VV.plot(col='time', col_wrap=2, robust=True, figsize=(10, 5))"
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html",
    "href": "unit_01/03_backscatter-coefficients.html",
    "title": "Normalizing Radar Cross Sections",
    "section": "",
    "text": "Microwave remote sensing is a technique that uses a certain type of electromagnetic waves (i.e., microwaves have a wavelength ranging from about one meter to one millimeter) to gather information about objects or areas on the ground. How does this work? Let´s break it down into points:\nRadar imaging systems can be expressed as a ratio between scattered power (\\(P_s\\)) and incident power (\\(P_i\\)), so that:\n\\[\\sigma = \\frac{P_s}{Pi} 4 \\pi R^2 \\, [m^2] \\quad \\text{,} \\quad  \\text{(equation 1)}\\]\nprovides a backscatter coefficient for distributed targets per given reference area, also known as a radar scattering cross section.\nIncreasing the target area on ground surfaces increases the total backscattered power by the same factor, and so the radar cross sections changes with the size of illuminated area. Radar cross sections are therefore often normalized by an approximation of the illuminated ground area \\(\\hat{A}\\), as follows:.\n\\[\\sigma^0 = \\sigma /  \\hat{A} \\, [1] \\quad \\text{.} \\quad  \\text{(equation 2)}\\]\nThis correction gives a more faithful description total backscattered power independent of the size of the illuminated area.\nBackscattering can, furthermore, be highly dependent on the angle of the incident energy when the target area is signified by reflective objects of different heights, such as trees. This phenomenon is called volume scattering and can be accounted for by again normalizing with the cosine of the local incidence angle:\n\\[\\gamma^0 = \\sigma^0 / \\cos{\\theta_i} \\, [1] \\quad \\text{.} \\quad  \\text{(equation 3)} \\]\nNow let’s have a look at some backscatter coefficients (\\(\\gamma^0\\) = gmr, \\(\\sigma^0\\) = sig0) and their respective local incidence angle (plia) over an area around the city of Modena, Italy."
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html#loading-normalized-backscatter-data",
    "href": "unit_01/03_backscatter-coefficients.html#loading-normalized-backscatter-data",
    "title": "Normalizing Radar Cross Sections",
    "section": "Loading Normalized Backscatter Data",
    "text": "Loading Normalized Backscatter Data\nWe first load the data with xarray, select an area of interest (AOI), and modify the variables gmr, sig0, and plia by dividing by 100. The latter step is necessary as the variables have been multiplied by 100, allowing more efficient storage of the data by decreasing the file size.\n\nimport xarray as xr\nimport rioxarray\nfrom pathlib import Path\nimport folium\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef _preprocess(x):\n    x = x / 100\n    return x.rename(\n        {\"band_data\": Path(x.encoding[\"source\"]).parent.stem}\n    ).squeeze(\"band\").drop_vars(\"band\")\n\nds = xr.open_mfdataset(\n    \"~/shared/datasets/rs/sentinel-1/A0105/EQUI7_EU010M/E047N012T1/**/*.tif\", \n    engine=\"rasterio\", \n    combine='nested', \n    preprocess=_preprocess\n    )\nds"
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html#area-of-interest",
    "href": "unit_01/03_backscatter-coefficients.html#area-of-interest",
    "title": "Normalizing Radar Cross Sections",
    "section": "Area of Interest",
    "text": "Area of Interest\nLet’s have a closer look at the area of interest by taking the bounding box of the xarray Dataset with rioxarray method transform_bounds and using this to define the viewing window on a leaflet map with folium.\n\nds_aoi = ds.sel(x=slice(4.78e6, 4.795e6), y=slice(1.28e6, 1.265e6)).compute()\n\nbbox = ds_aoi.rio.transform_bounds(\"EPSG:4326\")\n\nfolium.Map(\n    max_bounds=True,\n    location=[bbox[1] + (bbox[3] - bbox[1]) / 2, bbox[0] + (bbox[2] - bbox[0]) / 2],\n    min_lat=bbox[1],\n    max_lat=bbox[3],\n    min_lon=bbox[0],\n    max_lon=bbox[2],\n    zoom_control=False,\n    scrollWheelZoom=False,\n    dragging= False\n)\n\nFigure1: Map of the target area."
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html#visualizing-the-backscatter-coefficients",
    "href": "unit_01/03_backscatter-coefficients.html#visualizing-the-backscatter-coefficients",
    "title": "Normalizing Radar Cross Sections",
    "section": "Visualizing the Backscatter Coefficients",
    "text": "Visualizing the Backscatter Coefficients\nNow we can visualize the backscatter coefficients by using the xarray method plot, and by plotting these on a matplotlib figure with 2 axes (i.e., representing the subplots on a multipanel figure).\n\nfig, ax  = plt.subplots(1,2, figsize=(20,8))\n\nds_aoi.sig0.plot(ax=ax[0], robust=True)\nds_aoi.gmr.plot(ax=ax[1], robust=True)\n\nFigure 2: The \\(\\sigma^o\\) (left) and \\(\\gamma^o\\) (right) from the JupyterHub shared folder plotted on a 2D map\nNow let’s have a look at how the local incidence angles look like when plotted on 2D map and compare it with \\(\\sigma^o\\).\n\nfig, ax  = plt.subplots(1,2, figsize=(20,8))\n\nds_aoi.sig0.plot(ax=ax[0], robust=True)\nds_aoi.plia.plot(ax=ax[1], robust=True)\n\nFigure 3: The \\(\\sigma^o\\) (left) and local incidence angle (plia: right) from the JupyterHub shared folder plotted on a 2D map"
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html#calculating-gammao-with-numpy",
    "href": "unit_01/03_backscatter-coefficients.html#calculating-gammao-with-numpy",
    "title": "Normalizing Radar Cross Sections",
    "section": "Calculating \\(\\gamma^o\\) with numpy",
    "text": "Calculating \\(\\gamma^o\\) with numpy\nIt is clear by looking at the local incidence angles that the area is marked by topographically severe terrain. One of the data treatments corrected for the features related to rough terrain in Figure 1, whereas the other did not.\nLet’s now look at what the choice of backscatter coefficient (\\(\\sigma^0\\) or \\(\\gamma^0\\)) has on the data. We do this by first converting \\(\\sigma^0\\) to \\(\\gamma^0\\) with equation 3, like so:\n\n# linear scale\nds_lin = 10 ** (ds_aoi.sig0 / 10)\n# conversion to gamma\nds_gm = ds_lin / np.cos(np.radians(ds_aoi.plia))\n# dB scale\nds_aoi[\"gm\"] = 10 * np.log(ds_gm) \n\nfig, ax  = plt.subplots(1,2, figsize=(20,8))\n\nds_aoi.sig0.plot(ax=ax[0], robust=True)\nds_aoi.gm.plot(ax=ax[1], robust=True)\n\nFigure 4: The \\(\\sigma^o\\) (left) from the JupyterHub shared folder and the derived \\(\\gamma^0\\) plotted on a 2D map"
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients.html#references",
    "href": "unit_01/03_backscatter-coefficients.html#references",
    "title": "Normalizing Radar Cross Sections",
    "section": "References",
    "text": "References\nWoodhouse, Ian H., 2006, Introduction to Microwave Remote Sensing, Chapter 5: Microwaves in the Real World, pp 93–149, CRS Press"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JupyterHub Guide",
    "section": "",
    "text": "This is a short guide to create and run the notbooks and environments in the JupiterHub for the Master course Microwave Remote Sensing (120.030) at the TU Wien."
  },
  {
    "objectID": "index.html#starting-jupyterhub",
    "href": "index.html#starting-jupyterhub",
    "title": "JupyterHub Guide",
    "section": "Starting JupyterHub",
    "text": "Starting JupyterHub\nIn TUWEL, click on the JupyterHub icon , which redirects you to your own JupyterLab user environment. You should then select the Server for the Desktop Notebook from the Microwave Remote Sensing course. Be patient - this can take a couple of minutes.\n\n\n\nPlease note that all screenshots in this guide refer to the lecture 120.030 Microwave Remote Sensing (2024W), even if a few things are different for you, the overall functionality and interface remain the same."
  },
  {
    "objectID": "index.html#exploring-jupyterlab",
    "href": "index.html#exploring-jupyterlab",
    "title": "JupyterHub Guide",
    "section": "Exploring JupyterLab",
    "text": "Exploring JupyterLab\nWhen you start your server for the first time, your point of entry will be this starting page:\n\n\n\nIn the center, you have the Launcher where you can create Python or other files, play around with Jupyter Notebooks, store intermediate data. You can also open a Python console, a terminal, a text file, and many more. On the left, you can see your home directory where you have the folders lectures and shared. There might be other folders as well, but don’t be concerned about them.\n\n\nAfter some intense coding and analysis, it can happen that you have many terminal and notebook tabs open. However, simply closing them does not quit the processes and running kernels in the background. Therefore, we recommend that you tidy up your running processes after some time, which can be done as marked by the top-left circle. As an overview, the number of running kernels and terminals are always shown at the bottom-left corner."
  },
  {
    "objectID": "index.html#setup-of-exercise-environment",
    "href": "index.html#setup-of-exercise-environment",
    "title": "JupyterHub Guide",
    "section": "Setup of Exercise Environment",
    "text": "Setup of Exercise Environment\nTo use the Notebooks from the course, the following steps need to be taken:\n\nNavigate to the folder ~/shared/120.030-2024W/where you should find a Makefile and a README.md.\n\n\n\n\n\nIn this file, you should open a Terminal using the Launcher and run the command\nmake notebooks\nWait until the installation is finished - this can take a couple of minutes.\nThe notebooks and an environment, where the important packages and all their dependencies are included, have been installed for you.\nIt might be necessary to re-login to the JupyterHub to see it coming into effect.\nNow you should see a file named microwave-remote-sensing containing the notebooks and additional kernels in the Launcher of JupyterLab.\n\n\n\n\n\nSelect the kernel with the equivalent name as the .ipynb notebook to execute the notebook. For example, 01_lecture.ipynb requires the kernel 01_lecture for execution of the code blocks.\n\n\n\n\n \n\n\n\n\n\n\nIn case you could not select the correct server and could not find the folder from step one, you might be in the wrong server. You can change your server by selecting\nFile -&gt; Hub Control Panel\nand clicking on Stop My Server.\n\n\n\nThen press Start My Server and you should be able to select the Deeplearning Notebook for the Data Science Projects in Remote Sensing course."
  },
  {
    "objectID": "unit_01/03_backscatter-coefficients_exercise.html",
    "href": "unit_01/03_backscatter-coefficients_exercise.html",
    "title": "Template Notebook for TUW JupyterHub",
    "section": "",
    "text": "Calculate the absolute difference between \\(\\gamma^0\\) and \\(\\sigma^0\\) on a linear scale:\n\\[ A = \\gamma^0 - \\sigma^0 \\, [1] \\quad \\text{,}  \\]\nand the ratio on the Decibel range of \\(\\gamma^0\\) relative to \\(\\sigma^0\\), like so:\n\\[ R = \\frac{\\gamma^0}{\\sigma^0} \\, [dB] \\quad \\text{.} \\]\nWe can do these conversions by component wise mathematical operations with standard math operators on the xarray. Remember the rules of logarithms when performing these operations!\nUse the following Python modules and data for the calculations.\n\nimport xarray as xr\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\n\n\ndef _preprocess(x):\n    x = x * 0.01\n    return x.rename(\n        {\"band_data\": Path(x.encoding[\"source\"]).parent.stem}\n    ).squeeze(\"band\").drop_vars(\"band\")\n\nds = xr.open_mfdataset(\n    \"~/shared/datasets/rs/sentinel-1/A0105/EQUI7_EU010M/E047N012T1/**/*.tif\", \n    engine=\"rasterio\", \n    combine='nested', \n    preprocess=_preprocess\n    )\nds_aoi = ds.sel(x=slice(4.78e6, 4.795e6), y=slice(1.28e6, 1.265e6)).compute()\nds_aoi\n\n\nA = ...\n\n\nA.plot(robust=True, cmap=\"viridis\")\n\n\nR = ...\n\n\nR.plot(robust=True, cmap=\"viridis\")"
  },
  {
    "objectID": "unit_01/02_unit-conversions_exercise.html",
    "href": "unit_01/02_unit-conversions_exercise.html",
    "title": "Unit conversion exercise",
    "section": "",
    "text": "Find at least two different areas of interest (max extent of 1°), where you would expect significatly different backscatter values (Desert, forest, water, urban area, etc.). For each area of interest, compute the mean backscatter value across the area in dB for a time frame in summer and one in winter. Compare the results.\nSome functions have already been implemented for you. Fill out all the missing parts indicated by ... or # YOUR CODE HERE. You may adapt the code as you see fit. The final plot should show the average backscatter values for the different areas of interest in summer and winter and it should include the differences that occure when the mean across the scene is calculated with linear values or with logarithmic values.\n\n# Imports\nimport numpy as np\nimport pystac_client\nimport odc.stac\nimport matplotlib.pyplot as plt\nfrom odc.geo.geobox import GeoBox\nfrom dask.diagnostics import ProgressBar\nfrom rasterio.crs import CRS\nimport seaborn as sns\nimport pandas as pd"
  },
  {
    "objectID": "unit_01/02_unit-conversions_exercise.html#problem-statement",
    "href": "unit_01/02_unit-conversions_exercise.html#problem-statement",
    "title": "Unit conversion exercise",
    "section": "",
    "text": "Find at least two different areas of interest (max extent of 1°), where you would expect significatly different backscatter values (Desert, forest, water, urban area, etc.). For each area of interest, compute the mean backscatter value across the area in dB for a time frame in summer and one in winter. Compare the results.\nSome functions have already been implemented for you. Fill out all the missing parts indicated by ... or # YOUR CODE HERE. You may adapt the code as you see fit. The final plot should show the average backscatter values for the different areas of interest in summer and winter and it should include the differences that occure when the mean across the scene is calculated with linear values or with logarithmic values.\n\n# Imports\nimport numpy as np\nimport pystac_client\nimport odc.stac\nimport matplotlib.pyplot as plt\nfrom odc.geo.geobox import GeoBox\nfrom dask.diagnostics import ProgressBar\nfrom rasterio.crs import CRS\nimport seaborn as sns\nimport pandas as pd"
  },
  {
    "objectID": "unit_01/02_unit-conversions_exercise.html#conversion-formulas",
    "href": "unit_01/02_unit-conversions_exercise.html#conversion-formulas",
    "title": "Unit conversion exercise",
    "section": "Conversion formulas",
    "text": "Conversion formulas\nImplement the following formulas in your code: \\[\nD =  10  \\cdot \\log_{10} (I) = 10 \\cdot \\log_{10} (e) \\cdot \\ln (I)\\longrightarrow [dB]\n\\] \\[\nI = e^{\\frac{D}{10\\cdot \\log_{10}(e)}} = 10^{\\frac{D}{10}} \\longrightarrow [m^2m^{-2}]  \n\\]\n\ndef lin2db(x):\n    # YOUR CODE HERE\n    raise NotImplementedError()\n\ndef db2lin(x):\n    # YOUR CODE HERE\n    raise NotImplementedError()\n\n\ntimeframes = ['...', '...'] # add your timeframes here (e.g. '1971-01-01/1971-01-15')\naois = [(...,...), (...,...)] # add your AOIs as center points here (lat, lon) \n\n\n# NOTHING IN THIS CELL NEEDS TO BE CHANGED\ndef preprocess_data(dc, nodata_val=-9999, scale_factor=0.1, band='VV'):\n    '''\n    This function preprocesses the Sentinel-1 data by handling the nodata values and scaling the data.\n    Also it only loads the VV band.\n    '''\n    return (dc.where(dc != nodata_val)*scale_factor)[band].median(dim='time')\n\ndef load_data(aoi:list[tuple], timeframe:list[str], spatial_ext:float=0.125, epsg_code:int=4326, spatial_res:float=0.0002) -&gt; list:\n    '''\n    This function helps to load Sentinel-1 data from the EODC STAC API.\n\n    Params:\n    -------\n    - aoi (list[tuple]): List of AOIs as center points (lat, lon)\n    - timeframe (list[str]): List of timeframes in the format 'YYYY-MM-DD/YYYY-MM-DD'\n    - spatial_ext (float): Spatial extent around the center point\n    - epsg_code (int): EPSG code of the coordinate reference system\n    - spatial_res (float): Spatial resolution of the data\n\n    Returns:\n    -------\n    - list: List of xarray datasets\n    '''\n    epsg = CRS.from_epsg(epsg_code) \n    dx = spatial_res \n    datasets = []\n    for area in aoi:\n        for time in timeframe:\n            # Set the spatial extent\n            latmin, latmax = area[0] - spatial_ext, area[0] + spatial_ext\n            lonmin, lonmax = area[1] - spatial_ext, area[1] + spatial_ext\n            bounds = (lonmin, latmin, lonmax, latmax)\n\n            # Search items\n            items = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\").search(\n                # intersects=geom,\n                bbox=bounds,\n                collections=[\"SENTINEL1_SIG0_20M\"],\n                datetime=time,\n                limit=100,\n            ).item_collection()\n\n            # define a geobox for my region\n            geobox = GeoBox.from_bbox(bounds, crs=epsg, resolution=dx)\n\n            # lazily combine items\n            sig0_dc = odc.stac.stac_load(\n                items,\n                #bbox=bounds,\n                bands=[\"VV\", \"VH\"],\n                chunks={'time': 5, 'x': 600, 'y': 600},\n                epsg=epsg,\n                geobox=geobox,\n                resampling=\"bilinear\",\n            )\n            datasets.append(sig0_dc)\n    return datasets\n\nsig0_00, sig0_01, sig0_10, sig0_11 = load_data(aois, timeframes)\ndcs = [sig0_00, sig0_01, sig0_10, sig0_11]\n\n\n# NOTHING IN THIS CELL NEEDS TO BE CHANGED\ndef calc_mean(array, convert:bool= False) -&gt; dict:\n    '''\n    This function calculates the mean of the Sentinel-1 data.\n    '''\n    if convert:\n        array = db2lin(array) # convert to linear scale\n        mean_lin = array.mean().compute() # compute mean\n        mean = lin2db(mean_lin)# convert back to dB\n    else:\n        mean = array.mean().compute()\n\n    return float(mean)\n\n\nlabels = ['Loc1 Time1', 'Loc1 Time2', 'Loc2 Time1', 'Loc2 Time2'] # add your labels here\n\nmeans_dB = [calc_mean(...) for sig0 in dcs]\nmeans_lin_db = [calc_mean(..., convert=True) for sig0 in dcs]\n\n# Create a Pandas DataFrame\ndata = {\n    'Location': labels * 2,  # Repeat labels for each type (dB and linear)\n    'Mean Value (dB)': means_dB + means_lin_db,  # Concatenate the two lists of means\n    'Type': ['Original dB'] * len(labels) + ['Linear Converted'] * len(labels)  # Type indicator\n}\n\ndf = pd.DataFrame(data)\n\n\n#Startingpoint for your plot \nsns.barplot(data=df, x='...', y='...', hue='...')\n\n# YOUR CODE HERE (to make a nice and meaningful plot)\nraise NotImplementedError()"
  },
  {
    "objectID": "unit_01/01_discover-and-read _exercise.html",
    "href": "unit_01/01_discover-and-read _exercise.html",
    "title": "Discover and Read EO data - Exercise 1",
    "section": "",
    "text": "Synthetic Aperture Radar (SAR) is sensor sensitive to roughness of surface and di-electrical properties of it. Water bodies act as smooth mirror which reflects radar waves, so areas with minimal backscatter can be regarded as so. That being said, SAR interferometry is useful for monitoring floods, as change in backscatter signal indicates change in surface water levels. That can be done by observing the difference in surface water levels in some area of interest over period of several days, weeks or months.\nDisplay quicklook of Sentinel imagery over Thessaly region, Greece in September 2023.\n\nimport pystac_client\nimport folium\nfrom odc import stac as odc_stac\n\neodc_catalog = pystac_client.Client.open(\"https://stac.eodc.eu/api/v1\")\n\neodc_catalog"
  }
]